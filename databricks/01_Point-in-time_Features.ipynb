{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "439b17d6-29c2-45c2-bf3d-16fc342b5aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Point-in-time Lab Features\n",
    "\n",
    "To minimize the risk of conflating different ways to derive features, we are going to define the following categories of features:\n",
    " - **Point-in-Time Features**: A type of data join that ensures that features are derived using the first available observation before or at a given timestamp. Commonly, results are pivoted so that every lab will have it's own column.\n",
    " - **Sliding Window Features**: A way to aggregate events that happened within a specific rolling window (e.g., last 7 days, last 30 minutes). Typically these aggregates are numeric aggregates that return a scalar response like, `mean`, `min`, `max`,\n",
    " - **Events-Based Features**:\tFeatures are computed based on occurrences (one or many) of specific events before the observation point.\n",
    " - **Cohort-Based Features**: Features are generated based on historical groupings within a fixed observation window. The difference between Event-Based and Cohort-Based is the timestamp for each patient obeservation is the same, opposed an event-based where each patient event time is not shared. \n",
    "\n",
    "In this notebook, we'll explore writing a convenience function, `point_in_time_lab_features`, to do **Point-in-time Features** retrieval from the lab data we created in <a href=\"$./00_Data_Generation\" target=\"_blank\">00_Data_Generation</a> \n",
    ", `main.default.patient_lab`.\n",
    "\n",
    "**NOTE**: This notebook is intended to demonstrate an approach for quick discovery of features using [Tempo](https://databrickslabs.github.io/tempo/about/user-guide.html). The features that perform well in a given model may not necessarily be saved into a feature store the same way. Thus, [feature serving](https://docs.databricks.com/en/machine-learning/feature-store/feature-function-serving.html#what-is-databricks-feature-serving) is necessary for productionization of features, but isn't in scope of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d18533-4115-4de4-9f6c-3e245634816c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Dependencies"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install dbl-tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c80574ad-85e4-4e6e-be66-74aa0581e878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### labs_tsdf\n",
    "\n",
    "**TSDF** is a time-series wrapper for a Spark DataFrame. A TSDF contains additional metadata that identifies what column shall be used for time-series expressions and additional partition columns are declared to identify a single series. In our source table `main.default.patient_lab`, we will have a series for every (**patient_id**, **lab_type**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3675d55c-8424-4f52-b6c5-adaf1c02f975",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a TSDF"
    }
   },
   "outputs": [],
   "source": [
    "from tempo.tsdf import TSDF\n",
    "\n",
    "labs = spark.table(\"main.default.patient_lab\")\n",
    "labs_tsdf = TSDF(labs, ts_col=\"event_ts\", partition_cols = [\"patient_id\", \"lab_type\"])\n",
    "\n",
    "display(labs_tsdf.df.limit(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b291689f-1f24-4771-8963-134e0e29915d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Patient Event DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "# For as of joins to work, both tables need to be converted into a TSDF class\n",
    "patient_event = spark.table(\"main.default.patient_event\")\n",
    "patient_event_tsdf = TSDF(patient_event, ts_col=\"event_ts\", partition_cols = [\"patient_id\"])\n",
    "\n",
    "display(patient_event_tsdf.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d52e2885-d1bd-45d8-9961-be37dfb92237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### `point_in_time_lab_features`\n",
    "\n",
    "If we want to retrieve the most recent lab as of a given time from a table, this can be done at scale with performance using [asOfJoin](https://databrickslabs.github.io/tempo/references/tsdf.html#tempo.tsdf.TSDF.asofJoin).\n",
    "\n",
    "We'll go through a couple code snippets before we write the convenience function so that we can see the results of each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cfc5087-1e0c-484b-adc9-8d333a20a651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This will only return a single value for each patient. There is no declared tie breaker so it just picked one lab_type. \n",
    "# NOTE: This is not what we want.\n",
    "\n",
    "dat = patient_event_tsdf.asofJoin(right_tsdf=labs_tsdf,\n",
    "                                  left_prefix=\"patient\",\n",
    "                                  right_prefix=\"\")\n",
    "display(dat.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89ec616e-613c-4c48-9816-0f27bc3a3c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To get all specific labs we are interested in, we can explode by our desired labs array and conduct the same asOfJoin\n",
    "from pyspark.sql.functions import explode, lit\n",
    "\n",
    "lab_types = ['ua_ketones', 'ua_glucose']\n",
    "\n",
    "patient_event_labs = spark.table(\"main.default.patient_event\") \\\n",
    "                               .withColumn(\"lab_type\", explode(lit(lab_types)))\n",
    "patient_event_labs_tsdf = TSDF(patient_event_labs, ts_col=\"event_ts\", partition_cols = [\"patient_id\", \"lab_type\"])\n",
    "\n",
    "dat = patient_event_labs_tsdf.asofJoin(right_tsdf=labs_tsdf,\n",
    "                                       left_prefix=\"patient\",\n",
    "                                       right_prefix=\"\")\n",
    "display(dat.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf20acf3-d8ea-4649-acf8-c626c5e756eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Often, we'll want our data pivoted so that each lab_type is a column\n",
    "\n",
    "from pyspark.sql.functions import first_value\n",
    "\n",
    "group_by_cols = ['patient_id']\n",
    "group_by_cols += [ \"patient_\" + c for c in patient_event_labs.columns if c not in [\"patient_id\", \"lab_type\"]]\n",
    "\n",
    "p_dat = dat.df.groupBy(group_by_cols) \\\n",
    "              .pivot(\"lab_type\") \\\n",
    "              .agg(first_value(\"lab_value\"))\n",
    "\n",
    "display(p_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44443d12-1bc7-4934-8914-9d7606613152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Putting it all together, we can write a lab_as_of_features function such as:\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import explode, lit, first_value\n",
    "from tempo.tsdf import TSDF\n",
    "\n",
    "def lab_as_of_features(patient_event_df: DataFrame,\n",
    "                       lab_types: [str]):\n",
    "    \n",
    "    labs = spark.table(\"main.default.patient_lab\")\n",
    "    labs_tsdf = TSDF(labs, ts_col=\"event_ts\", partition_cols = [\"patient_id\", \"lab_type\"])\n",
    "\n",
    "    patient_event_labs = patient_event_df.withColumn(\"lab_type\", explode(lit(lab_types)))\n",
    "    patient_event_labs_tsdf = TSDF(patient_event_labs, ts_col=\"event_ts\", partition_cols = [\"patient_id\", \"lab_type\"])\n",
    "\n",
    "    patient_as_of_labs = patient_event_labs_tsdf.asofJoin(right_tsdf=labs_tsdf,\n",
    "                                                          left_prefix=\"patient\",\n",
    "                                                          right_prefix=\"\")\n",
    "\n",
    "    group_by_cols = ['patient_id'] + \\\n",
    "                    [ \"patient_\" + c for c in patient_event_labs.columns if c not in [\"patient_id\", \"lab_type\"]]\n",
    "    \n",
    "    return patient_as_of_labs.df.groupBy(group_by_cols) \\\n",
    "                                .pivot(\"lab_type\") \\\n",
    "                                .agg(first_value(\"lab_value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0be94dd-e163-4873-8a65-6d14a82511bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dat = lab_as_of_features(patient_event_df=spark.table(\"main.default.patient_event\"),\n",
    "                         lab_types=['ua_ketones', 'ua_glucose'])\n",
    "display(dat)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8187281512173624,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Point-in-time_Features",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
