{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a027418-d1c0-42e0-bd87-4a27d3fa9ec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Sliding Window Features\n",
    "\n",
    "To minimize the risk of conflating different ways to derive features, we are going to define the following categories of features:\n",
    " - **Point-in-Time Features**: A type of data join that ensures that features are derived using the first available observation before or at a given timestamp. Commonly, results are pivoted so that every lab will have it's own column.\n",
    " - **Sliding Window Features**: A way to aggregate events that happened within a specific rolling window (e.g., last 7 days, last 30 minutes). Typically these aggregates are numeric aggregates that return a scalar response like, `mean`, `min`, `max`,\n",
    " - **Events-Based Features**:\tFeatures are computed based on occurrences (one or many) of specific events before the observation point.\n",
    " - **Cohort-Based Features**: Features are generated based on historical groupings within a fixed observation window. The difference between Event-Based and Cohort-Based is the timestamp for each patient obeservation is the same, opposed an event-based where each patient event time is not shared. \n",
    "\n",
    "In this notebook, we'll explore writing a convenience function, `sliding_window_numeric_aggregates`, to do **Sliding Window Features** retrieval from the lab data we created in <a href=\"$./00_Data_Generation\" target=\"_blank\">00_Data_Generation</a> \n",
    ", `main.default.patient_lab`.\n",
    "\n",
    "**NOTE**: We are going to write `sliding_window_numeric_aggregates` with the naming convention `<agg_class_name>_<num_days>_<lab_type>`. This convention is necessary because we will need to dynamically create the feature names in a way that there is no name collisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1223c02d-dbd8-4c71-9452-bef40146a3c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### `sliding_window_numeric_aggregates`\n",
    "\n",
    "Sliding window features are the most commonly thought of feature aggregates. While it is possible to write aggregate functions that can reduce strings or any other supported spark column types, the largest number of built-in aggregrate functions are actually written for numeric values. Therefore so that we can demonstrate using these functions by just passing existing built in functions, we will write the function where it pre-supposes that the `lab_value` can be coearsed into a FLOAT type. Thus, we will be working with `ua_ph` labs which is numeric.\n",
    "\n",
    "So that we can see all the transforms that will go into creating `sliding_window_numeric_aggregates`, we will show each step and put the steps together in a final function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b884c531-782a-4f2c-b9d7-c98638979d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to do a range join, we'll define our look back window size in days\n",
    "from pyspark.sql.functions import col, to_timestamp, expr\n",
    "from pyspark.sql.functions import explode, lit, date_sub\n",
    "\n",
    "# This is approximately one month and 9 months\n",
    "windows_in_days = [1*30, 9*30]\n",
    "\n",
    "# We'll use lab_types to filter for only the labs of interest\n",
    "lab_types = ['ua_ph', ]\n",
    "\n",
    "patient_lab = spark.table(\"main.default.patient_lab\").alias('pl')\n",
    "patient_event = spark.table(\"main.default.patient_event\").alias('pe')\n",
    "\n",
    "patient_event_labs = patient_event.withColumn(\"window_days\", explode(lit(windows_in_days))) \\\n",
    "                                  .withColumnRenamed(\"event_ts\",\"end_window_ts\") \\\n",
    "                                  .withColumn(\"start_window_ts\", date_sub(col(\"end_window_ts\"), col(\"window_days\")).cast(\"TIMESTAMP\")) \\\n",
    "                                  .join(patient_lab.filter(col(\"pl.lab_type\").isin(lab_types)),\n",
    "                                        (patient_lab.patient_id == patient_event.patient_id) &\n",
    "                                        (patient_lab.event_ts.between(col(\"start_window_ts\"), col(\"end_window_ts\"))),\n",
    "                                        \"leftouter\") \\\n",
    "                                  .drop(col(\"pl.patient_id\"), \"start_window_ts\") \\\n",
    "                                  .withColumn('lab_value', col('lab_value').cast(\"FLOAT\"))\n",
    "\n",
    "display(patient_event_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c48706a4-378c-43cb-bd17-725ce2993ca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, struct, col, concat_ws\n",
    "from pyspark.sql.functions import min, max, mean\n",
    "\n",
    "agg_funcs = [min, max, mean]\n",
    "\n",
    "aggs = [x(\"lab_value\").alias(f\"{x.__name__}\") for x in agg_funcs]\n",
    "\n",
    "agg_cols = [f\"{x.__name__}\" for x in agg_funcs]\n",
    "\n",
    "# Group by patient_id and lab_type, and collect event_ts and lab_value into an array of structs\n",
    "grouped_labs = patient_event_labs.withColumn(\"days_lab\", concat_ws(\"_\", \"lab_type\",\"window_days\")) \\\n",
    "                                 .groupBy(\"patient_id\", \"days_lab\", \"end_window_ts\") \\\n",
    "                                 .agg(*aggs) \\\n",
    "                                 .withColumn(\"aggregates\", struct(*[col(c) for c in agg_cols])) \\\n",
    "                                 .drop(*agg_cols)\n",
    "\n",
    "display(grouped_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2528c92f-034c-47ab-ac7e-ecceab478370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first_value\n",
    "\n",
    "pivot_labs = grouped_labs.withColumnRenamed(\"end_window_ts\",\"event_ts\") \\\n",
    "                         .groupBy(\"patient_id\", \"event_ts\") \\\n",
    "                         .pivot(\"days_lab\") \\\n",
    "                         .agg(first_value(\"aggregates\"))\n",
    "\n",
    "display(pivot_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd48e0a-70fc-4da6-a200-99015fdb5f37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from itertools import product\n",
    "\n",
    "window_cols = [c for c in pivot_labs.columns if c not in ['patient_id', 'event_ts']]\n",
    "\n",
    "feat_cols = [col(f'{w}.{a}').alias(f'{w}_{a}') for w, a in product(window_cols, agg_cols)]\n",
    "\n",
    "rslt = pivot_labs.select(\"patient_id\", \"event_ts\", *feat_cols)\n",
    "\n",
    "display(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57e32ecc-d5f4-4b2f-b642-9fc54b0ba0e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Putting it all together, we can write a sliding_window_numeric_aggregates function such as:\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, to_timestamp, expr, explode, lit, date_sub, collect_list, struct, col, concat_ws, first_value\n",
    "from pyspark.sql.functions import min, max, mean\n",
    "from typing import Callable\n",
    "from pyspark.sql import Column\n",
    "from itertools import product\n",
    "\n",
    "def sliding_window_numeric_aggregates(patient_event_df: DataFrame,\n",
    "                                      agg_funcs:[Callable[[str], Column]],\n",
    "                                      lab_types: [str],\n",
    "                                      windows_in_days: int):\n",
    "    \n",
    "    patient_lab = spark.table(\"main.default.patient_lab\").alias('pl')\n",
    "\n",
    "    patient_event_labs = patient_event_df.alias('pe') \\\n",
    "                                         .withColumn(\"window_days\", explode(lit(windows_in_days))) \\\n",
    "                                         .withColumnRenamed(\"event_ts\",\"end_window_ts\") \\\n",
    "                                         .withColumn(\"start_window_ts\", date_sub(col(\"end_window_ts\"), col(\"window_days\")).cast(\"TIMESTAMP\")) \\\n",
    "                                         .join(patient_lab.filter(col(\"pl.lab_type\").isin(lab_types)),\n",
    "                                               (patient_lab.patient_id == patient_event_df.patient_id) &\n",
    "                                               (patient_lab.event_ts.between(col(\"start_window_ts\"), col(\"end_window_ts\"))),\n",
    "                                               \"leftouter\") \\\n",
    "                                         .drop(col(\"pl.patient_id\"), \"start_window_ts\") \\\n",
    "                                         .withColumn('lab_value', col('lab_value').cast(\"FLOAT\"))\n",
    "    \n",
    "    aggs = [x(\"lab_value\").alias(f\"{x.__name__}\") for x in agg_funcs]\n",
    "    agg_cols = [f\"{x.__name__}\" for x in agg_funcs]\n",
    "\n",
    "    grouped_labs = patient_event_labs.withColumn(\"days_lab\", concat_ws(\"_\", \"lab_type\",\"window_days\")) \\\n",
    "                                     .groupBy(\"patient_id\", \"days_lab\", \"end_window_ts\") \\\n",
    "                                     .agg(*aggs) \\\n",
    "                                     .withColumn(\"aggregates\", struct(*[col(c) for c in agg_cols])) \\\n",
    "                                     .drop(*agg_cols)\n",
    "\n",
    "    pivot_labs = grouped_labs.withColumnRenamed(\"end_window_ts\",\"event_ts\") \\\n",
    "                             .groupBy(\"patient_id\", \"event_ts\") \\\n",
    "                             .pivot(\"days_lab\") \\\n",
    "                             .agg(first_value(\"aggregates\"))\n",
    "\n",
    "    window_cols = [c for c in pivot_labs.columns if c not in ['patient_id', 'event_ts']]\n",
    "    feat_cols = [col(f'{w}.{a}').alias(f'{w}_{a}') for w, a in product(window_cols, agg_cols)]\n",
    "\n",
    "    rslt = pivot_labs.select(\"patient_id\", \"event_ts\", *feat_cols)\n",
    "\n",
    "    return rslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c1f5069-43d2-4531-a78b-697c1591176c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dat = sliding_window_numeric_aggregates(patient_event_df=spark.table(\"main.default.patient_event\"),\n",
    "                                        agg_funcs=[min, max, mean],\n",
    "                                        lab_types=['ua_ph',],\n",
    "                                        windows_in_days=[1*30, 9*30])\n",
    "\n",
    "display(dat)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Sliding_Window_Features",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
