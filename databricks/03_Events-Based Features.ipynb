{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "138dd94c-0987-40bd-9b6c-2cc60fbbca83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Events-Based Features\n",
    "\n",
    "To minimize the risk of conflating different ways to derive features, we are going to define the following categories of features:\n",
    " - **Point-in-Time Features**: A type of data join that ensures that features are derived using the first available observation before or at a given timestamp. Commonly, results are pivoted so that every lab will have it's own column.\n",
    " - **Sliding Window Features**: A way to aggregate events that happened within a specific rolling window (e.g., last 7 days, last 30 minutes). Typically these aggregates are numeric aggregates that return a scalar response like, `mean`, `min`, `max`,\n",
    " - **Events-Based Features**:\tFeatures are computed based on occurrences (one or many) of specific events before the observation point.\n",
    " - **Cohort-Based Features**: Features are generated based on historical groupings within a fixed observation window. The difference between Event-Based and Cohort-Based is the timestamp for each patient obeservation is the same, opposed an event-based where each patient event time is not shared. \n",
    "\n",
    "In this notebook, we'll explore writing a convenience function, `events_based_lab_features`, to do **Events-Based Features** retrieval from the lab data we created in <a href=\"$./00_Data_Generation\" target=\"_blank\">00_Data_Generation</a> \n",
    ", `main.default.patient_lab`.\n",
    "\n",
    "**NOTE**: Events based features differ from Sliding Window Based features in how they might be deployed into a production environement. Sliding Window Features commonly have their aggregates as scalars stored in feature serving, where Events-Based features may return only an intermediate form of an array struct where further feature transform is done in the consuming model pipeline. This will be more clear when evaluting the outputs of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c59de3e-9012-4b9e-a9fc-d3c78ba70e86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### `events_based_lab_features`\n",
    "\n",
    "Similar to sliding window features, there isn't a benefit to using Tempo. For building our convenience function it is more straight forward to use a range join. However, unlinke sliding window features, we will be returning a complex struct and the consuming model pipeline will convert to the scalars used for the model family input. This approach is helpful when there is a need to have the time of request as part of the feature calculation and therefore the aggregates can't be pre-calculated in the feature store.\n",
    "\n",
    "**NOTE**: When writing our function, we are going to write it so that the window is defined in seconds. Offsets by hours, days, weeks, months, quarters, year should have their own convenience functions since a typical interpretation would be to look back to the start of interval which will yield a different result than the equivalent seconds calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9584ab64-7e1d-4932-9592-c341d7dc5423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to do a range join, we define our look back window size in seconds\n",
    "from pyspark.sql.functions import col, to_timestamp, expr\n",
    "\n",
    "# This is an approximate 6 month window in seconds\n",
    "window_size_in_seconds = 6*30*24*60*60\n",
    "\n",
    "# We'll use lab_types to filter for only the labs of interest\n",
    "lab_types = ['ua_protein', 'ua_ketones']\n",
    "\n",
    "patient_lab = spark.table(\"main.default.patient_lab\").alias('pl')\n",
    "patient_event = spark.table(\"main.default.patient_event\").alias('pe')\n",
    "\n",
    "window_labs = patient_event.withColumnRenamed(\"event_ts\",\"end_window_ts\") \\\n",
    "                           .withColumn(\"start_window_ts\", expr(f\"end_window_ts - INTERVAL {window_size_in_seconds} seconds\")) \\\n",
    "                           .join(patient_lab.filter(col(\"pl.lab_type\").isin(lab_types)),\n",
    "                                 (patient_lab.patient_id == patient_event.patient_id) &\n",
    "                                 (patient_lab.event_ts.between(col(\"start_window_ts\"), col(\"end_window_ts\"))),\n",
    "                                 \"leftouter\") \\\n",
    "                           .drop(col(\"pl.patient_id\"), \"start_window_ts\")\n",
    "                   \n",
    "display(window_labs.limit(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0beaa30-c042-424b-b874-9f3eb9dbe030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, struct, col\n",
    "\n",
    "# Group by patient_id and lab_type, and collect event_ts and lab_value into an array of structs\n",
    "grouped_labs = window_labs.groupBy(\"patient_id\", \"lab_type\", \"end_window_ts\") \\\n",
    "                          .agg(collect_list(struct(\"event_ts\", \"lab_value\")).alias(\"labs\"))\n",
    "\n",
    "display(grouped_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2785e03-e6ff-4435-ad67-1467deefe8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first_value\n",
    "\n",
    "pivot_labs = grouped_labs.withColumnRenamed(\"end_window_ts\",\"event_ts\") \\\n",
    "                         .groupBy(\"patient_id\", \"event_ts\") \\\n",
    "                         .pivot(\"lab_type\") \\\n",
    "                         .agg(first_value(\"labs\"))\n",
    "\n",
    "display(pivot_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4744560-97e3-4611-b4ff-1fac493ca5c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Putting it all together, we can write a events_based_lab_features function such as:\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import expr, col, collect_list, struct, first_value\n",
    "\n",
    "def events_based_lab_features(patient_event_df: DataFrame,\n",
    "                              lab_types: [str],\n",
    "                              window_size_in_seconds: int):\n",
    "    patient_lab = spark.table(\"main.default.patient_lab\").alias('pl')\n",
    "\n",
    "    window_labs = patient_event_df.alias('pe') \\\n",
    "                                  .withColumnRenamed(\"event_ts\",\"end_window_ts\") \\\n",
    "                                  .withColumn(\"start_window_ts\", expr(f\"end_window_ts - INTERVAL {window_size_in_seconds} seconds\")) \\\n",
    "                                  .join(patient_lab.filter(col(\"pl.lab_type\").isin(lab_types)),\n",
    "                                        (patient_lab.patient_id == patient_event_df.patient_id) &\n",
    "                                        (patient_lab.event_ts.between(col(\"start_window_ts\"), col(\"end_window_ts\"))),\n",
    "                                        \"leftouter\") \\\n",
    "                                  .drop(col(\"pl.patient_id\"), \"start_window_ts\")\n",
    "    grouped_labs = window_labs.groupBy(\"patient_id\", \"lab_type\", \"end_window_ts\") \\\n",
    "                              .agg(collect_list(struct(\"event_ts\", \"lab_value\")).alias(\"labs\"))\n",
    "    pivot_labs = grouped_labs.withColumnRenamed(\"end_window_ts\",\"event_ts\") \\\n",
    "                         .groupBy(\"patient_id\", \"event_ts\") \\\n",
    "                         .pivot(\"lab_type\") \\\n",
    "                         .agg(first_value(\"labs\"))\n",
    "    return pivot_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b3c007c-6194-4f34-ae53-a99fc0b89057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dat =  events_based_lab_features(patient_event_df=spark.table(\"main.default.patient_event\"),\n",
    "                                 lab_types=['ua_protein', 'ua_ketones'],\n",
    "                                 window_size_in_seconds=6*30*24*60*60)\n",
    "\n",
    "display(dat)                                "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7440668743606107,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Events-Based Features",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
