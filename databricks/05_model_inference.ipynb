{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa21a60b-608d-474b-8b9e-54c651b21c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Batch Model Inference\n",
    "\n",
    "Once we have registered our model in the model registry we will download and use that model for spark batch inference. \n",
    "\n",
    "This repo is going over how to create ts aggregate functions which produces features from a source table labs. There is no assurance that the feature transforms from model training will be the same as the feature transforms used during inference. This must be monitored by the development team for accuracy. Databricks does offer an enterprise solution for managing feature consistancy by adopting [Databricks Feature engineering and serving](https://docs.databricks.com/en/machine-learning/feature-store/index.html).\n",
    "\n",
    "Aggregate timeseries data that we are doing in this repo is more complicated than the examples shown in Databricks documentation for a couple reasons:\n",
    " - Time series data usually has late arriving data that will requires inserts and updates opposed to typical insert only pattern\n",
    " - Sparse feature entries (meaning only creating a feature observation when there is a change to features) complicates the addition of new feature windows. When a new feature window is introduced a record must be updated for the ts_event when an observation enters the window **and** leaves the window. \n",
    " - There is a databricks private preview for ts aggregate features. However, it is currently written to create a dense feature table that has features rewritten for all possible intervals, ie. day. This is still an efficient feature serving strategy since tables can be compressed efficiently.\n",
    "\n",
    " **Recommendation**: If only batch inference is required and Near Real Time inference is not, consider using the following batch inference pattern. Once the aggregate timeseries functionality is GA and there is a business requirement for NRT inference, then make the investment in building out a solution with databricks feature serving.\n",
    "\n",
    " **NOTE**: In the script below, we are pulling by registered model verion. However, the intended governanace pattern is to apply an alias to the version of interest and pull via alias. Check out [deploy model aliases](https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/index.html?utm_source=chatgpt.com#deploy-models-using-aliases) when ready to adopt alias to identify current production model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e34bf03-8c55-47fc-ab8d-f9925c39b4d3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run Setup"
    }
   },
   "outputs": [],
   "source": [
    "%run ./_setup/setup_patient_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af621b16-0311-4be5-909d-c9e112b04d5e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Retrieve  Registered Model"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pyspark.sql.functions import struct, col\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "uc_model_name = \"main.default.patient_lab_sick\"\n",
    "uc_model_version = \"3\"\n",
    "\n",
    "registered_model_uri =  f\"models:/{uc_model_name}/{uc_model_version}\"\n",
    "\n",
    "# Load model as a Spark UDF, this will make at scale batch prediction in spark possible\n",
    "model = mlflow.pyfunc.spark_udf(spark, model_uri=registered_model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e448a50-3048-484d-977e-44f43c8ef5fb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Batch Inference Features"
    }
   },
   "outputs": [],
   "source": [
    "from patient_features.agg_func import sliding_window_numeric_aggregates\n",
    "from pyspark.sql.functions import min, max, mean\n",
    "\n",
    "patient_event_df = spark.read.table('main.default.patient_event')\n",
    "patient_lab = spark.table(\"main.default.patient_lab\")\n",
    "\n",
    "batch_features = sliding_window_numeric_aggregates(patient_event_df=patient_event_df,\n",
    "                                                   patient_lab=patient_lab,\n",
    "                                                   agg_funcs=[min, max, mean],\n",
    "                                                   lab_types=['ua_ph',],\n",
    "                                                   windows_in_days=[12*30, 9*30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0102f5d4-23ee-48d8-8f2f-790884e57c72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run Spark Batch Prediction"
    }
   },
   "outputs": [],
   "source": [
    "batch_predict = batch_features.withColumn('predict', model(struct(*map(col, batch_features.columns))))\n",
    "display(batch_predict)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_model_inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
